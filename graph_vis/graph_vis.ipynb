{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a notebook to visualize the relationship between the wiki articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "count_vectorizer = text.CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                                strip_accents = 'unicode', # works \n",
    "                                stop_words = 'english', # works\n",
    "                                lowercase = True, # works\n",
    "                                max_df = 0.5, # works\n",
    "                                min_df = 10) # works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_phys = pd.read_csv(\"../nlp_clean/wiki_phys_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_phys.drop_duplicates(\"Name\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1479, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_phys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_count_vectorized=count_vectorizer.fit_transform(wiki_phys['Text'].values.astype('U'))\n",
    "\n",
    "lda_model_count=LDA(n_components=26\n",
    "              ,max_iter=100,learning_method='batch').fit(data_count_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cos_sim\n",
    "\n",
    "def top_n_index_sorted(array, top_n):\n",
    "    # get the index of top 10 values in the array\n",
    "    index = np.argpartition(array,-top_n)[-top_n:]\n",
    "    \n",
    "    # return the index of the sorted top_n values \n",
    "    return index[np.argsort(array[index])]\n",
    "\n",
    "def top10_recommend_index(index,model,data,top_n):\n",
    "    # transform the data using the model\n",
    "    lda_vectors=model.transform(data)\n",
    "    # calculate the cos similarity from the lda vectors \n",
    "    similarity=cos_sim(lda_vectors)\n",
    "    \n",
    "    top_n_index=top_n_index_sorted(similarity[index,],top_n+1)\n",
    "    \n",
    "    # get the decreasing similarity index\n",
    "    return top_n_index[::-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top10_recommend_1 = top10_recommend_index(1,lda_model_count,data_count_vectorized,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to generate edge list\n",
    "def add_edges_from_recommendation(index,model,data,text_vectors):\n",
    "    top10_recommended = top10_recommend_index(index,model,text_vectors,10)\n",
    "    \n",
    "    edge_list = list()\n",
    "    original_article = data['Name'].iloc[index]\n",
    "    for recommend_index in top10_recommended[1:]:\n",
    "        node_name= data['Name'].iloc[recommend_index]\n",
    "        edge_list.append((original_article,node_name))\n",
    "    \n",
    "    return edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia:FAQ/Categorization', 'List of physics journals'),\n",
       " ('Wikipedia:FAQ/Categorization', 'Category:Physics stubs'),\n",
       " ('Wikipedia:FAQ/Categorization', 'List of Slovenian physicists'),\n",
       " ('Wikipedia:FAQ/Categorization', 'Template:Physics-stub'),\n",
       " ('Wikipedia:FAQ/Categorization', 'Template:Acoustics-stub'),\n",
       " ('Wikipedia:FAQ/Categorization', 'Template:Biophysics-stub'),\n",
       " ('Wikipedia:FAQ/Categorization',\n",
       "  'International Association of Physics Students'),\n",
       " ('Wikipedia:FAQ/Categorization',\n",
       "  \"Ukrainian Physicists' Tournament for University Students\"),\n",
       " ('Wikipedia:FAQ/Categorization', 'CESRA'),\n",
       " ('Wikipedia:FAQ/Categorization',\n",
       "  'Physics Instructional Resource Association')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_edges_from_recommendation(0,lda_model_count,wiki_phys,data_count_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 1150/1479 [13:55<04:10,  1.31it/s]"
     ]
    }
   ],
   "source": [
    "# for the progress bar\n",
    "from tqdm import tqdm\n",
    "wiki_graph = nx.Graph()\n",
    "\n",
    "for index in tqdm(range(wiki_phys.shape[0])):\n",
    "    wiki_graph.add_edges_from(add_edges_from_recommendation(index,lda_model_count,wiki_phys,data_count_vectorized))\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "nx.draw_networkx(wiki_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
